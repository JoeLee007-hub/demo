import streamlit as st
from dotenv import load_dotenv
load_dotenv()

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))

import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from datetime import datetime, timedelta
import json
import logging
from pathlib import Path
from scipy import stats
from typing import Dict, List, Optional, Tuple, Any

from src.core.data_manager import DataManager
from src.core.readability import ReadabilityAnalyzer
from src.utils.helpers import *
from src.utils.formatters import *

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# é¡µé¢é…ç½®
st.set_page_config(page_title="æ•°æ®åˆ†æ - AIå„¿ç«¥æ•…äº‹åˆ›ä½œç³»ç»Ÿ", layout="wide")

# è¯­è¨€è®¾ç½®
if 'lang' not in st.session_state:
    st.session_state['lang'] = 'en'

def T(zh, en):
    return zh if st.session_state['lang']=='zh' else en

# ç»Ÿè®¡åˆ†ææ¨¡å—
class StatisticalAnalysis:
    """ç»Ÿè®¡åˆ†æç±»ï¼Œæä¾›å®Œæ•´çš„æ•°æ®åˆ†æåŠŸèƒ½"""
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def analyze_stories(self, df: pd.DataFrame) -> Dict[str, Any]:
        """å®Œæ•´ç»Ÿè®¡åˆ†æåŠŸèƒ½
        
        Args:
            df: æ•…äº‹æ•°æ®DataFrame
            
        Returns:
            åŒ…å«å„ç§ç»Ÿè®¡åˆ†æç»“æœçš„å­—å…¸
        """
        if df.empty:
            self.logger.warning("Empty DataFrame provided for analysis")
            return {}
        
        try:
            results = {
                'descriptive_stats': self._descriptive_analysis(df),
                'correlation_analysis': self._correlation_analysis(df),
                'hypothesis_testing': self._hypothesis_testing(df),
                'distribution_analysis': self._analyze_distributions(df)
            }
            self.logger.info(f"Statistical analysis completed for {len(df)} records")
            return results
        except Exception as e:
            self.logger.error(f"Error in statistical analysis: {str(e)}")
            return {}
    
    def _descriptive_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æè¿°æ€§ç»Ÿè®¡åˆ†æ
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            åŒ…å«åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯å’Œå¼‚å¸¸å€¼æ£€æµ‹ç»“æœçš„å­—å…¸
        """
        numeric_cols = ['flesch_score', 'word_count', 'sentence_count']
        available_cols = [col for col in numeric_cols if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
        
        if not available_cols:
            self.logger.warning("No numeric columns available for descriptive analysis")
            return {}
        
        try:
            return {
                'basic_stats': df[available_cols].describe(),
                'outliers': self._detect_outliers(df, available_cols),
                'missing_values': df[available_cols].isnull().sum().to_dict()
            }
        except Exception as e:
            self.logger.error(f"Error in descriptive analysis: {str(e)}")
            return {}
    
    def _correlation_analysis(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç›¸å…³æ€§åˆ†æ
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            åŒ…å«Pearsonå’ŒSpearmanç›¸å…³ç³»æ•°çš„å­—å…¸
        """
        numeric_cols = ['flesch_score', 'word_count', 'sentence_count', 'age']
        available_cols = [col for col in numeric_cols if col in df.columns and pd.api.types.is_numeric_dtype(df[col])]
        
        if len(available_cols) < 2:
            self.logger.warning("Insufficient numeric columns for correlation analysis")
            return {}
        
        try:
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            clean_df = df[available_cols].dropna()
            if clean_df.empty:
                return {}
                
            return {
                'pearson_correlations': clean_df.corr(method='pearson'),
                'spearman_correlations': clean_df.corr(method='spearman'),
                'sample_size': len(clean_df)
            }
        except Exception as e:
            self.logger.error(f"Error in correlation analysis: {str(e)}")
            return {}
    
    def _hypothesis_testing(self, df):
        """å‡è®¾æ£€éªŒ"""
        results = {}
        
        # æ¯”è¾ƒä¸åŒprompté£æ ¼çš„å¯è¯»æ€§
        if 'prompt_style' in df.columns and 'flesch_score' in df.columns:
            results['prompt_style_anova'] = self._compare_prompt_styles(df)
        
        # æ¯”è¾ƒä¸åŒå¹´é¾„ç»„çš„å¯è¯»æ€§
        if 'age' in df.columns and 'flesch_score' in df.columns:
            results['age_group_comparison'] = self._compare_age_groups(df)
            
        return results
    
    def _compare_prompt_styles(self, df: pd.DataFrame) -> Dict[str, Any]:
        """æ¯”è¾ƒä¸åŒprompté£æ ¼çš„æ•ˆæœ
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            ANOVAæ£€éªŒç»“æœå­—å…¸
        """
        try:
            # æ•°æ®éªŒè¯
            if 'prompt_style' not in df.columns or 'flesch_score' not in df.columns:
                return {}
            
            # æ¸…ç†æ•°æ®
            clean_df = df[['prompt_style', 'flesch_score']].dropna()
            if clean_df.empty:
                return {}
            
            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®è¿›è¡Œæ¯”è¾ƒ
            style_groups = clean_df.groupby('prompt_style')['flesch_score'].apply(lambda x: list(x))
            valid_groups = [group for group in style_groups.values if len(group) >= 2]  # æ¯ç»„è‡³å°‘2ä¸ªæ ·æœ¬
            
            if len(valid_groups) >= 2:
                # è½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
                valid_groups = [np.array(group, dtype=float) for group in valid_groups]
                
                f_stat, p_value = stats.f_oneway(*valid_groups)
                
                # è®¡ç®—æ•ˆåº”é‡ (eta squared)
                all_data = np.concatenate(valid_groups)
                total_var = np.var(all_data, ddof=1)
                group_means = [np.mean(group) for group in valid_groups]
                group_sizes = [len(group) for group in valid_groups]
                overall_mean = np.mean(all_data)
                
                between_var = sum(size * (mean - overall_mean)**2 for size, mean in zip(group_sizes, group_means))
                within_var = sum((len(group)-1) * np.var(group, ddof=1) for group in valid_groups)
                eta_squared = between_var / (between_var + within_var) if (between_var + within_var) > 0 else 0
                
                return {
                    'f_statistic': float(f_stat),
                    'p_value': float(p_value),
                    'significant': p_value < 0.05,
                    'eta_squared': float(eta_squared),
                    'group_count': len(valid_groups),
                    'total_samples': sum(group_sizes)
                }
        except Exception as e:
            self.logger.error(f"Prompt style comparison failed: {str(e)}")
        return {}
    
    def _compare_age_groups(self, df):
        """æ¯”è¾ƒä¸åŒå¹´é¾„ç»„"""
        try:
            age_groups = df.groupby('age')['flesch_score'].apply(list).to_dict()
            if len(age_groups) >= 2:
                groups = [scores for scores in age_groups.values() if len(scores) > 1]
                if len(groups) >= 2:
                    f_stat, p_value = stats.f_oneway(*groups)
                    return {
                        'f_statistic': f_stat,
                        'p_value': p_value,
                        'significant': p_value < 0.05
                    }
        except Exception as e:
            st.warning(f"Age group comparison failed: {str(e)}")
        return {}
    
    def _analyze_distributions(self, df):
        """åˆ†ææ•°æ®åˆ†å¸ƒç‰¹å¾"""
        results = {}
        numeric_cols = ['flesch_score', 'word_count', 'sentence_count']
        
        for col in numeric_cols:
            if col in df.columns and not df[col].empty:
                try:
                    data = df[col].dropna()
                    if len(data) > 3:
                        results[col] = {
                            'skewness': stats.skew(data),
                            'kurtosis': stats.kurtosis(data),
                            'normality_test': stats.shapiro(data) if len(data) <= 5000 else None
                        }
                except Exception as e:
                    continue
        return results
    
    def _detect_outliers(self, df, columns):
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        outliers = {}
        for col in columns:
            if col in df.columns:
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                outliers[col] = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]
        return outliers

# è¯„ä¼°æŒ‡æ ‡ä½“ç³»
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡ç±»ï¼Œæä¾›ç³»ç»Ÿæ€§èƒ½çš„å¤šç»´åº¦è¯„ä¼°"""
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def comprehensive_evaluation(self, stories_df: pd.DataFrame) -> Dict[str, Any]:
        """ç»¼åˆè¯„ä¼°ç³»ç»Ÿæ€§èƒ½
        
        Args:
            stories_df: æ•…äº‹æ•°æ®æ¡†
            
        Returns:
            åŒ…å«å„ç»´åº¦è¯„ä¼°ç»“æœçš„å­—å…¸
        """
        evaluation = {}
        
        try:
            # å†…å®¹è´¨é‡è¯„ä¼°
            if not stories_df.empty:
                evaluation['content_quality'] = self._evaluate_content_quality(stories_df)
            
            # å¯è¯»æ€§è¯„ä¼°
            if not stories_df.empty:
                evaluation['readability_metrics'] = self._evaluate_readability(stories_df)
            
            # Promptæ•ˆæœè¯„ä¼°
            if not stories_df.empty:
                evaluation['prompt_effectiveness'] = self._evaluate_prompt_effectiveness(stories_df)
            
            self.logger.info(f"Comprehensive evaluation completed with {len(evaluation)} metrics")
            return evaluation
            
        except Exception as e:
            self.logger.error(f"Error in comprehensive evaluation: {str(e)}")
            return {}
    
    def _evaluate_content_quality(self, df):
        """å†…å®¹è´¨é‡è¯„ä¼°"""
        quality_metrics = {}
        
        if 'word_count' in df.columns:
            quality_metrics['avg_word_count'] = df['word_count'].mean()
            quality_metrics['word_count_std'] = df['word_count'].std()
        
        if 'sentence_count' in df.columns:
            quality_metrics['avg_sentence_count'] = df['sentence_count'].mean()
            quality_metrics['sentence_count_std'] = df['sentence_count'].std()
        
        # è¯æ±‡å¤šæ ·æ€§ï¼ˆåŸºäºè¯æ•°å’Œå¥æ•°çš„æ¯”ä¾‹ï¼‰
        if 'word_count' in df.columns and 'sentence_count' in df.columns:
            df_clean = df.dropna(subset=['word_count', 'sentence_count'])
            if not df_clean.empty:
                quality_metrics['avg_words_per_sentence'] = (df_clean['word_count'] / df_clean['sentence_count']).mean()
        
        return quality_metrics
    
    def _evaluate_readability(self, df):
        """å¯è¯»æ€§è¯„ä¼°"""
        readability_metrics = {}
        
        if 'flesch_score' in df.columns:
            readability_metrics['flesch_reading_ease'] = df['flesch_score'].describe()
            readability_metrics['age_appropriateness'] = self._check_age_appropriateness(df)
        
        return readability_metrics
    
    def _evaluate_satisfaction(self, df):
        """ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°"""
        satisfaction_metrics = {}
        
        if 'rating' in df.columns:
            satisfaction_metrics['overall_rating'] = df['rating'].describe()
            satisfaction_metrics['satisfaction_distribution'] = df['rating'].value_counts().to_dict()
            satisfaction_metrics['positive_feedback_rate'] = (df['rating'] >= 4).mean()
        
        return satisfaction_metrics
    
    def _evaluate_prompt_effectiveness(self, stories_df):
        """Promptæ•ˆæœè¯„ä¼°"""
        effectiveness_metrics = {}
        
        if 'prompt_style' in stories_df.columns:
            # ä¸åŒé£æ ¼çš„å¯è¯»æ€§å·®å¼‚
            style_readability = stories_df.groupby('prompt_style')['flesch_score'].agg(['mean', 'std', 'count'])
            effectiveness_metrics['style_readability'] = style_readability.to_dict()
            
            # ä¸åŒé£æ ¼çš„è¯æ•°å·®å¼‚
            if 'word_count' in stories_df.columns:
                style_wordcount = stories_df.groupby('prompt_style')['word_count'].agg(['mean', 'std', 'count'])
                effectiveness_metrics['style_wordcount'] = style_wordcount.to_dict()
        
        return effectiveness_metrics
    
    def _check_age_appropriateness(self, df):
        """æ£€æŸ¥å¹´é¾„é€‚å®œæ€§"""
        if 'flesch_score' in df.columns and 'age' in df.columns:
            age_readability = df.groupby('age')['flesch_score'].mean().to_dict()
            return age_readability
        return {}

# é¡µé¢æ ‡é¢˜
title_col, lang_col = st.columns([8, 1])
with title_col:
    st.markdown(
        f"<h1 style='font-size:2.3em;margin-bottom:0.2em;'>{T('æ•°æ®åˆ†æä¸å¯è§†åŒ–','Data Analysis & Visualization')}</h1>",
        unsafe_allow_html=True
    )
with lang_col:
    lang_map = {"ä¸­æ–‡": "zh", "English": "en"}
    lang_display = st.selectbox(
        "", 
        options=list(lang_map.keys()),
        index=1 if st.session_state.get('lang', 'en') == 'en' else 0
    )
    st.session_state['lang'] = lang_map[lang_display]

# æ•°æ®åŠ è½½å‡½æ•°
@st.cache_data(ttl=300)  # 5åˆ†é’Ÿç¼“å­˜
def load_stories_data(group_filter: str = "all") -> List[Dict[str, Any]]:
    """åŠ è½½æ•…äº‹æ•°æ®
    
    Args:
        group_filter: ç»„è¿‡æ»¤å™¨ ("all", "group_a", "group_b", "group_c", "group_d")
    
    Returns:
        æ•…äº‹æ•°æ®åˆ—è¡¨
    """
    try:
        data_manager = DataManager()
        stories = data_manager.list_stories()
        
        # æ ¹æ®ç»„è¿‡æ»¤æ•°æ®
        if group_filter != "all":
            filtered_stories = []
            for story in stories:
                params = story.get('parameters', {})
                experiment_group = params.get('experiment_group', '').lower()
                
                # å¦‚æœæ²¡æœ‰experiment_groupå­—æ®µï¼Œè·³è¿‡è¯¥æ•…äº‹
                if not experiment_group:
                    continue
                
                # æ ¹æ®å®éªŒç»„åè¿‡æ»¤
                if experiment_group == group_filter:
                    filtered_stories.append(story)
            
            stories = filtered_stories
        
        logger.info(f"Successfully loaded {len(stories)} stories for {group_filter}")
        return stories
    except Exception as e:
        error_msg = f'Failed to load stories data: {str(e)}'
        logger.error(error_msg)
        st.error(T(f'åŠ è½½æ•…äº‹æ•°æ®å¤±è´¥ï¼š{str(e)}', error_msg))
        return []

@st.cache_data(ttl=300)  # 5åˆ†é’Ÿç¼“å­˜
def load_feedback_data() -> List[Dict[str, Any]]:
    """åŠ è½½åé¦ˆæ•°æ®
    
    Returns:
        åé¦ˆæ•°æ®åˆ—è¡¨
    """
    try:
        from src.config import FEEDBACK_DIR
        feedback_dir = Path(FEEDBACK_DIR)
        feedback_data = []
        
        if feedback_dir.exists():
            json_files = list(feedback_dir.glob('*.json'))
            for filepath in json_files:
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        feedback_data.append(data)
                except (json.JSONDecodeError, IOError) as file_error:
                    logger.warning(f"Failed to load feedback file {filepath}: {str(file_error)}")
                    continue
        
        logger.info(f"Successfully loaded {len(feedback_data)} feedback entries")
        return feedback_data
        
    except Exception as e:
        error_msg = f'Failed to load feedback data: {str(e)}'
        logger.error(error_msg)
        st.error(T(f'åŠ è½½åé¦ˆæ•°æ®å¤±è´¥ï¼š{str(e)}', error_msg))
        return []

# æ•°æ®å¤„ç†å‡½æ•°
@st.cache_data
def process_stories_data(stories: List[Dict[str, Any]]) -> pd.DataFrame:
    """å¤„ç†æ•…äº‹æ•°æ®ä¸ºDataFrame
    
    Args:
        stories: æ•…äº‹æ•°æ®åˆ—è¡¨
        
    Returns:
        å¤„ç†åçš„DataFrame
    """
    if not stories:
        logger.warning("No stories provided for processing")
        return pd.DataFrame()
    
    processed_data = []
    analyzer = ReadabilityAnalyzer()
    failed_count = 0
    
    for i, story in enumerate(stories):
        try:
            # åŸºæœ¬ä¿¡æ¯éªŒè¯
            story_info = {
                'story_id': story.get('story_id', f'story_{i}'),
                'content': story.get('content', ''),
                'created_at': story.get('created_at', datetime.now().isoformat())
            }
            
            # å‚æ•°ä¿¡æ¯
            params = story.get('parameters', {})
            story_info.update({
                'age': max(1, min(18, params.get('age', 6))),  # å¹´é¾„èŒƒå›´éªŒè¯
                'character': params.get('character', 'unknown'),
                'theme': params.get('theme', 'unknown'),
                'prompt_style': params.get('prompt_style', 'unknown'),
                'word_limit': max(50, min(1000, params.get('word_limit', 300))),  # è¯æ•°é™åˆ¶éªŒè¯
                'model': params.get('model', 'unknown')
            })
            
            # å¯è¯»æ€§åˆ†æ
            if story_info['content'].strip():
                try:
                    readability = analyzer.analyze(story_info['content'])
                    story_info.update({
                        'flesch_score': max(0, min(100, readability.get('Flesch Reading Ease', 0))),
                        'sentence_count': max(1, readability.get('Sentence Count', 1)),
                        'word_count': max(1, readability.get('Word Count', 1)),
                        'avg_sentence_length': readability.get('Average Sentence Length', 0),
                        'avg_word_length': readability.get('Average Word Length', 0),
                        'recommended_age': readability.get('Recommended Age Range', 'unknown')
                    })
                except Exception as readability_error:
                    logger.warning(f"Readability analysis failed for story {story_info['story_id']}: {str(readability_error)}")
                    # ä½¿ç”¨ç®€å•çš„å¤‡ç”¨è®¡ç®—
                    content = story_info['content']
                    word_count = len(content.split())
                    sentence_count = max(1, content.count('.') + content.count('!') + content.count('?'))
                    
                    story_info.update({
                        'flesch_score': 50,  # é»˜è®¤ä¸­ç­‰å¯è¯»æ€§
                        'sentence_count': sentence_count,
                        'word_count': word_count,
                        'avg_sentence_length': word_count / sentence_count if sentence_count > 0 else 0,
                        'avg_word_length': sum(len(word) for word in content.split()) / word_count if word_count > 0 else 0,
                        'recommended_age': 'unknown'
                    })
            else:
                # ç©ºå†…å®¹å¤„ç†
                story_info.update({
                    'flesch_score': 0,
                    'sentence_count': 0,
                    'word_count': 0,
                    'avg_sentence_length': 0,
                    'avg_word_length': 0,
                    'recommended_age': 'unknown'
                })
            
            processed_data.append(story_info)
            
        except Exception as e:
            failed_count += 1
            logger.error(f'Error processing story {i}: {str(e)}')
            if failed_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                st.warning(T(f'å¤„ç†æ•…äº‹æ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}', f'Error processing story data: {str(e)}'))
    
    if failed_count > 0:
        logger.warning(f"Failed to process {failed_count} out of {len(stories)} stories")
    
    df = pd.DataFrame(processed_data)
    logger.info(f"Successfully processed {len(df)} stories")
    return df

@st.cache_data
def process_feedback_data(feedback_data: List[Dict[str, Any]]) -> pd.DataFrame:
    """å¤„ç†åé¦ˆæ•°æ®ä¸ºDataFrame
    
    Args:
        feedback_data: åé¦ˆæ•°æ®åˆ—è¡¨
        
    Returns:
        å¤„ç†åçš„DataFrame
    """
    if not feedback_data:
        logger.warning("No feedback data provided for processing")
        return pd.DataFrame()
    
    processed_data = []
    failed_count = 0
    
    for i, feedback in enumerate(feedback_data):
        try:
            # æ•°æ®éªŒè¯å’Œæ¸…ç†
            rating = feedback.get('rating', 0)
            if not isinstance(rating, (int, float)) or rating < 0 or rating > 5:
                rating = 0
            
            feedback_info = {
                'feedback_id': feedback.get('feedback_id', f'feedback_{i}'),
                'story_id': feedback.get('story_id', 'unknown'),
                'rating': rating,
                'comment': str(feedback.get('comment', '')).strip(),
                'timestamp': feedback.get('timestamp', datetime.now().isoformat())
            }
            
            # é¢å¤–ä¿¡æ¯éªŒè¯
            extra = feedback.get('extra', {})
            age = extra.get('age', 6)
            if not isinstance(age, (int, float)) or age < 1 or age > 18:
                age = 6
            
            word_limit = extra.get('word_limit', 300)
            if not isinstance(word_limit, (int, float)) or word_limit < 50 or word_limit > 1000:
                word_limit = 300
            
            feedback_info.update({
                'theme': extra.get('theme', 'unknown'),
                'character': extra.get('character', 'unknown'),
                'age': age,
                'model': extra.get('model', 'unknown'),
                'prompt_style': extra.get('prompt_style', 'unknown'),
                'word_limit': word_limit
            })
            
            processed_data.append(feedback_info)
            
        except Exception as e:
            failed_count += 1
            logger.error(f'Error processing feedback {i}: {str(e)}')
            if failed_count <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªé”™è¯¯
                st.warning(T(f'å¤„ç†åé¦ˆæ•°æ®æ—¶å‡ºé”™ï¼š{str(e)}', f'Error processing feedback data: {str(e)}'))
    
    if failed_count > 0:
        logger.warning(f"Failed to process {failed_count} out of {len(feedback_data)} feedback entries")
    
    df = pd.DataFrame(processed_data)
    logger.info(f"Successfully processed {len(df)} feedback entries")
    return df

# æ“ä½œæŒ‰é’®
col1, col2, col3 = st.columns([1, 1, 4])
with col1:
    if st.button(T("è¿”å›ä¸»é¡µ", "Back to Home")):
        st.switch_page("streamlit_app.py")
with col2:
    if st.button(T("åˆ·æ–°æ•°æ®", "Refresh Data")):
        try:
            # æ¸…é™¤ç¼“å­˜
            st.cache_data.clear()
            logger.info("Data cache cleared by user")
            st.success(T('æ•°æ®å·²åˆ·æ–°', 'Data refreshed successfully'))
            # é‡æ–°åŠ è½½æ•°æ®æ—¶ä¿æŒå½“å‰é€‰æ‹©çš„ç»„
            if 'selected_group' in locals():
                st.session_state['selected_group'] = selected_group
            st.rerun()
        except Exception as e:
            logger.error(f"Error refreshing data: {str(e)}")
            st.error(T('åˆ·æ–°æ•°æ®å¤±è´¥', 'Failed to refresh data'))

# ä¸»è¦å†…å®¹
st.markdown("---")

# å®éªŒç»„é€‰æ‹©
st.markdown(f"## {T('å®éªŒæ•°æ®é€‰æ‹©', 'Experiment Data Selection')}")

group_options = {
    T("å…¨éƒ¨æ•°æ®", "All Data"): "all",
    T("Group A - Friendship (Little Fox)", "Group A - Friendship (Little Fox)"): "group_a",
    T("Group B - Cooperation (Little Ant)", "Group B - Cooperation (Little Ant)"): "group_b", 
    T("Group C - Environmental Protection (Little Bear)", "Group C - Environmental Protection (Little Bear)"): "group_c",
    T("Group D - Friendship (Little Mouse)", "Group D - Friendship (Little Mouse)"): "group_d"
}

selected_group_display = st.selectbox(
    T("é€‰æ‹©è¦åˆ†æçš„å®éªŒç»„:", "Select experiment group to analyze:"),
    options=list(group_options.keys()),
    index=0,
    help=T("é€‰æ‹©ç‰¹å®šçš„å®éªŒç»„è¿›è¡Œåˆ†æï¼Œæˆ–é€‰æ‹©å…¨éƒ¨æ•°æ®è¿›è¡Œç»¼åˆåˆ†æ", "Select a specific experiment group for analysis, or choose all data for comprehensive analysis")
)

selected_group = group_options[selected_group_display]

# æ˜¾ç¤ºé€‰æ‹©çš„ç»„ä¿¡æ¯
if selected_group != "all":
    group_info = {
        "group_a": T("ä¸»é¢˜ï¼šå‹è°Š | è§’è‰²ï¼šå°ç‹ç‹¸ | ç›®æ ‡ï¼šåŸ¹å…»å‹è°Šä»·å€¼è§‚", "Theme: Friendship | Character: Little Fox | Goal: Foster friendship values"),
        "group_b": T("ä¸»é¢˜ï¼šåˆä½œ | è§’è‰²ï¼šå°èš‚èš | ç›®æ ‡ï¼šåŸ¹å…»åˆä½œç²¾ç¥", "Theme: Cooperation | Character: Little Ant | Goal: Foster cooperation spirit"),
        "group_c": T("ä¸»é¢˜ï¼šç¯ä¿ | è§’è‰²ï¼šå°ç†Š | ç›®æ ‡ï¼šåŸ¹å…»ç¯ä¿æ„è¯†", "Theme: Environmental Protection | Character: Little Bear | Goal: Foster environmental awareness"),
        "group_d": T("ä¸»é¢˜ï¼šå‹è°Š | è§’è‰²ï¼šå°è€é¼  | ç›®æ ‡ï¼šåŸ¹å…»å‹è°Šä»·å€¼è§‚", "Theme: Friendship | Character: Little Mouse | Goal: Foster friendship values")
    }
    st.info(f"ğŸ“Š {T('å½“å‰åˆ†æç»„', 'Current Analysis Group')}: {group_info[selected_group]}")

st.markdown("---")

# æ•°æ®åŠ è½½
try:
    with st.spinner(T('æ­£åœ¨åŠ è½½æ•°æ®...', 'Loading data...')):
        stories_data = load_stories_data(selected_group)
        
        if stories_data:
            stories_df = process_stories_data(stories_data)
        else:
            stories_df = pd.DataFrame()
            if selected_group == "all":
                st.warning(T('æœªæ‰¾åˆ°æ•…äº‹æ•°æ®', 'No story data found'))
            else:
                st.warning(T(f'æœªæ‰¾åˆ°{selected_group_display}çš„æ•°æ®', f'No data found for {selected_group_display}'))
            
except Exception as e:
    logger.error(f"Critical error in data loading: {str(e)}")
    st.error(T(f'æ•°æ®åŠ è½½å¤±è´¥ï¼š{str(e)}', f'Data loading failed: {str(e)}'))
    st.stop()

# æ˜¾ç¤ºæœ€åæ›´æ–°æ—¶é—´
st.caption(T(f'æ•°æ®æœ€åæ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}', 
             f'Data last updated: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}'))

# åˆå§‹åŒ–åˆ†ææ¨¡å—
stat_analyzer = StatisticalAnalysis()
evaluator = EvaluationMetrics()

# æ‰§è¡Œç»Ÿè®¡åˆ†æ
if not stories_df.empty:
    statistical_results = stat_analyzer.analyze_stories(stories_df)
else:
    statistical_results = {}

# æ‰§è¡Œç»¼åˆè¯„ä¼°
if not stories_df.empty:
    evaluation_results = evaluator.comprehensive_evaluation(stories_df)
else:
    evaluation_results = {}

# æ•°æ®æ¦‚è§ˆ
if not stories_df.empty:
    st.markdown(f"## {T('æ•°æ®æ¦‚è§ˆ', 'Data Overview')}")
    
    col1, col2 = st.columns(2)
    
    with col1:
        story_count = len(stories_df) if not stories_df.empty else 0
        st.metric(
            T('æ€»æ•…äº‹æ•°', 'Total Stories'),
            story_count,
            delta=None if story_count == 0 else f"+{story_count}"
        )
    
    with col2:
        if not stories_df.empty and 'flesch_score' in stories_df.columns:
            valid_scores = stories_df['flesch_score'].dropna()
            if not valid_scores.empty:
                avg_readability = valid_scores.mean()
                readability_level = "Good" if avg_readability >= 60 else "Fair" if avg_readability >= 30 else "Poor"
                st.metric(
                    T('å¹³å‡å¯è¯»æ€§', 'Average Readability'),
                    f"{avg_readability:.1f} {readability_level}",
                    delta=f"{avg_readability - 50:.1f}" if avg_readability != 50 else None
                )
            else:
                st.metric(T('å¹³å‡å¯è¯»æ€§', 'Average Readability'), "N/A")
        else:
            st.metric(T('å¹³å‡å¯è¯»æ€§', 'Average Readability'), "N/A")
else:
    st.warning(T('æš‚æ— æ•°æ®å¯æ˜¾ç¤º', 'No data available to display'))

# ç»Ÿè®¡åˆ†ææ¦‚è§ˆ
if statistical_results and not stories_df.empty:
    st.markdown("---")
    st.markdown(f"## {T('ç»Ÿè®¡åˆ†ææ¦‚è§ˆ', 'Statistical Analysis Overview')}")
    
    try:
        # æè¿°æ€§ç»Ÿè®¡
        if 'descriptive_stats' in statistical_results and statistical_results['descriptive_stats']:
            with st.expander(T("æè¿°æ€§ç»Ÿè®¡", "Descriptive Statistics"), expanded=True):
                desc_stats = statistical_results['descriptive_stats']
                
                if 'basic_stats' in desc_stats and not desc_stats['basic_stats'].empty:
                    st.write(T("**åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:**", "**Basic Statistics:**"))
                    st.dataframe(desc_stats['basic_stats'].round(3), use_container_width=True)
                
                # ç¼ºå¤±å€¼ä¿¡æ¯
                if 'missing_values' in desc_stats:
                    missing_data = desc_stats['missing_values']
                    if any(count > 0 for count in missing_data.values()):
                        st.write(T("**æ•°æ®è´¨é‡:**", "**Data Quality:**"))
                        col1, col2 = st.columns(2)
                        with col1:
                            for col, count in missing_data.items():
                                if count > 0:
                                    st.warning(f"{col}: {count} {T('ä¸ªç¼ºå¤±å€¼', 'missing values')}")
                        with col2:
                            total_records = len(stories_df)
                            completeness = {col: (total_records - count) / total_records * 100 
                                          for col, count in missing_data.items()}
                            st.write(T("æ•°æ®å®Œæ•´æ€§:", "Data Completeness:"))
                            for col, pct in completeness.items():
                                st.write(f"{col}: {pct:.1f}%")
                
                # å¼‚å¸¸å€¼æ£€æµ‹
                if 'outliers' in desc_stats:
                    outliers = desc_stats['outliers']
                    if outliers and any(count > 0 for count in outliers.values()):
                        st.write(T("**å¼‚å¸¸å€¼æ£€æµ‹:**", "**Outlier Detection:**"))
                        outlier_cols = st.columns(len(outliers))
                        for i, (col, count) in enumerate(outliers.items()):
                            if count > 0:
                                outlier_cols[i % len(outlier_cols)].metric(f"{col}", f"{count} {T('ä¸ª', 'outliers')}")
    except Exception as e:
        logger.error(f"Error in descriptive statistics display: {str(e)}")
        st.error(T("æ˜¾ç¤ºæè¿°æ€§ç»Ÿè®¡æ—¶å‡ºé”™", "Error displaying descriptive statistics"))
    
    # ç›¸å…³æ€§åˆ†æ
    if 'correlation_analysis' in statistical_results and statistical_results['correlation_analysis']:
        with st.expander(T("ç›¸å…³æ€§åˆ†æ", "Correlation Analysis")):
            corr_analysis = statistical_results['correlation_analysis']
            sample_size = corr_analysis.get('sample_size', 0)
            
            if sample_size > 0:
                st.info(T(f"åˆ†ææ ·æœ¬é‡: {sample_size}", f"Analysis sample size: {sample_size}"))
            
            if 'pearson_correlations' in corr_analysis:
                corr_matrix = corr_analysis['pearson_correlations']
                if not corr_matrix.empty:
                    st.write(T("**çš®å°”é€Šç›¸å…³ç³»æ•°:**", "**Pearson Correlations:**"))
                    
                    # åˆ›å»ºç›¸å…³æ€§çƒ­åŠ›å›¾
                    fig = px.imshow(
                        corr_matrix,
                        text_auto='.3f',
                        aspect="auto",
                        color_continuous_scale='RdBu_r',
                        title=T("ç›¸å…³æ€§çŸ©é˜µ", "Correlation Matrix"),
                        zmin=-1, zmax=1
                    )
                    fig.update_layout(
                        height=400,
                        title_x=0.5,
                        font=dict(size=12)
                    )
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # æ˜¾ç¤ºå¼ºç›¸å…³æ€§
                    strong_corrs = []
                    for i in range(len(corr_matrix.columns)):
                        for j in range(i+1, len(corr_matrix.columns)):
                            corr_val = corr_matrix.iloc[i, j]
                            if abs(corr_val) > 0.5:  # å¼ºç›¸å…³æ€§é˜ˆå€¼
                                strong_corrs.append({
                                    'variables': f"{corr_matrix.columns[i]} - {corr_matrix.columns[j]}",
                                    'correlation': corr_val,
                                    'strength': T('å¼ºæ­£ç›¸å…³', 'Strong Positive') if corr_val > 0.5 else T('å¼ºè´Ÿç›¸å…³', 'Strong Negative')
                                })
                    
                    if strong_corrs:
                        st.write(T("**å¼ºç›¸å…³æ€§ (|r| > 0.5):**", "**Strong Correlations (|r| > 0.5):**"))
                        for corr in strong_corrs:
                            st.write(f"â€¢ {corr['variables']}: {corr['correlation']:.3f} ({corr['strength']})")
            
            if 'spearman_correlations' in corr_analysis:
                st.write(T("**æ–¯çš®å°”æ›¼ç­‰çº§ç›¸å…³:**", "**Spearman Rank Correlation:**"))
                spearman_matrix = corr_analysis['spearman_correlations']
                if not spearman_matrix.empty:
                    st.dataframe(spearman_matrix.round(3), use_container_width=True)
    
    # å‡è®¾æ£€éªŒç»“æœ
    if 'hypothesis_testing' in statistical_results and statistical_results['hypothesis_testing']:
        with st.expander(T("å‡è®¾æ£€éªŒ", "Hypothesis Testing")):
            hypothesis_results = statistical_results['hypothesis_testing']
            
            # Prompté£æ ¼æ¯”è¾ƒ
            if 'prompt_style_comparison' in hypothesis_results and hypothesis_results['prompt_style_comparison']:
                prompt_result = hypothesis_results['prompt_style_comparison']
                st.write(T("**Prompté£æ ¼æ•ˆæœæ¯”è¾ƒ (ANOVA):**", "**Prompt Style Effect Comparison (ANOVA):**"))
                
                col1, col2, col3 = st.columns(3)
                with col1:
                    f_stat = prompt_result.get('f_statistic', 0)
                    st.metric("Fç»Ÿè®¡é‡", f"{f_stat:.4f}")
                with col2:
                    p_val = prompt_result.get('p_value', 1)
                    st.metric("på€¼", f"{p_val:.4f}")
                with col3:
                    eta_sq = prompt_result.get('eta_squared', 0)
                    st.metric("æ•ˆåº”é‡ (Î·Â²)", f"{eta_sq:.3f}")
                
                # æ˜¾ç¤ºç»“æœè§£é‡Š
                if prompt_result.get('significant', False):
                    st.success(T("ç»“æœæ˜¾è‘— (p < 0.05) - ä¸åŒPrompté£æ ¼é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚", 
                               "Significant result (p < 0.05) - Significant differences between prompt styles"))
                    
                    # æ•ˆåº”é‡è§£é‡Š
                    if eta_sq >= 0.14:
                        effect_desc = T("å¤§æ•ˆåº”", "Large effect")
                    elif eta_sq >= 0.06:
                        effect_desc = T("ä¸­ç­‰æ•ˆåº”", "Medium effect")
                    else:
                        effect_desc = T("å°æ•ˆåº”", "Small effect")
                    st.info(f"{T('æ•ˆåº”é‡å¤§å°', 'Effect size')}: {effect_desc}")
                else:
                    st.info(T("ç»“æœä¸æ˜¾è‘— (p â‰¥ 0.05) - ä¸åŒPrompté£æ ¼é—´æ— æ˜¾è‘—å·®å¼‚", 
                             "Non-significant result (p â‰¥ 0.05) - No significant differences between prompt styles"))
                
                # æ˜¾ç¤ºç»„ä¿¡æ¯
                if 'group_count' in prompt_result and 'total_samples' in prompt_result:
                    st.caption(f"{T('åˆ†æç»„æ•°', 'Groups analyzed')}: {prompt_result['group_count']}, "
                             f"{T('æ€»æ ·æœ¬é‡', 'Total samples')}: {prompt_result['total_samples']}")
            
            # å¹´é¾„ç»„æ¯”è¾ƒ
            if 'age_group_comparison' in hypothesis_results and hypothesis_results['age_group_comparison']:
                age_result = hypothesis_results['age_group_comparison']
                st.write(T("**å¹´é¾„ç»„æ¯”è¾ƒ (ANOVA):**", "**Age Group Comparison (ANOVA):**"))
                
                col1, col2 = st.columns(2)
                with col1:
                    f_stat = age_result.get('f_statistic', 0)
                    st.metric("Fç»Ÿè®¡é‡", f"{f_stat:.4f}")
                with col2:
                    p_val = age_result.get('p_value', 1)
                    st.metric("på€¼", f"{p_val:.4f}")
                
                if age_result.get('significant', False):
                    st.success(T("ç»“æœæ˜¾è‘— (p < 0.05) - ä¸åŒå¹´é¾„ç»„é—´å­˜åœ¨æ˜¾è‘—å·®å¼‚", 
                               "Significant result (p < 0.05) - Significant differences between age groups"))
                else:
                    st.info(T("ç»“æœä¸æ˜¾è‘— (p â‰¥ 0.05) - ä¸åŒå¹´é¾„ç»„é—´æ— æ˜¾è‘—å·®å¼‚", 
                             "Non-significant result (p â‰¥ 0.05) - No significant differences between age groups"))
            
            # å¦‚æœæ²¡æœ‰å¯ç”¨çš„å‡è®¾æ£€éªŒç»“æœ
            if not any(key in hypothesis_results for key in ['prompt_style_comparison', 'age_group_comparison']):
                st.warning(T("æš‚æ— å¯ç”¨çš„å‡è®¾æ£€éªŒç»“æœ", "No hypothesis testing results available"))

# ç»¼åˆè¯„ä¼°æŒ‡æ ‡
if evaluation_results:
    st.markdown("---")
    st.markdown(f"## {T('ç»¼åˆè¯„ä¼°æŒ‡æ ‡', 'Comprehensive Evaluation Metrics')}")
    
    # å†…å®¹è´¨é‡è¯„ä¼°
    if 'content_quality' in evaluation_results:
        with st.expander(T("å†…å®¹è´¨é‡è¯„ä¼°", "Content Quality Assessment")):
            quality = evaluation_results['content_quality']
            col1, col2 = st.columns(2)
            
            with col1:
                if 'avg_word_count' in quality:
                    st.metric(T("å¹³å‡è¯æ•°", "Average Word Count"), f"{quality['avg_word_count']:.1f}")
                if 'avg_sentence_count' in quality:
                    st.metric(T("å¹³å‡å¥æ•°", "Average Sentence Count"), f"{quality['avg_sentence_count']:.1f}")
            
            with col2:
                if 'avg_words_per_sentence' in quality:
                    st.metric(T("å¹³å‡å¥é•¿", "Average Sentence Length"), f"{quality['avg_words_per_sentence']:.1f}")
    
    # ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°
    if 'user_satisfaction' in evaluation_results:
        with st.expander(T("ç”¨æˆ·æ»¡æ„åº¦è¯„ä¼°", "User Satisfaction Assessment")):
            satisfaction = evaluation_results['user_satisfaction']
            
            if 'positive_feedback_rate' in satisfaction:
                st.metric(
                    T("æ­£é¢åé¦ˆç‡", "Positive Feedback Rate"),
                    f"{satisfaction['positive_feedback_rate']:.1%}"
                )
            
            if 'satisfaction_distribution' in satisfaction:
                dist_data = satisfaction['satisfaction_distribution']
                fig = px.bar(
                    x=list(dist_data.keys()),
                    y=list(dist_data.values()),
                    title=T("è¯„åˆ†åˆ†å¸ƒ", "Rating Distribution"),
                    labels={'x': T('è¯„åˆ†', 'Rating'), 'y': T('æ•°é‡', 'Count')}
                )
                st.plotly_chart(fig, use_container_width=True)
    
    # Promptæ•ˆæœè¯„ä¼°
    if 'prompt_effectiveness' in evaluation_results:
        with st.expander(T("Promptæ•ˆæœè¯„ä¼°", "Prompt Effectiveness Assessment")):
            effectiveness = evaluation_results['prompt_effectiveness']
            
            if 'style_readability' in effectiveness:
                st.write(T("ä¸åŒé£æ ¼çš„å¯è¯»æ€§å¯¹æ¯”:", "Readability Comparison by Style:"))
                style_data = effectiveness['style_readability']
                if 'mean' in style_data:
                    readability_df = pd.DataFrame(style_data)
                    fig = px.bar(
                        readability_df,
                        y=readability_df.index,
                        x='mean',
                        error_x='std',
                        orientation='h',
                        title=T("å„é£æ ¼å¹³å‡å¯è¯»æ€§", "Average Readability by Style"),
                        labels={'mean': T('å¹³å‡å¯è¯»æ€§åˆ†æ•°', 'Average Readability Score'), 'y': T('Prompté£æ ¼', 'Prompt Style')}
                    )
                    st.plotly_chart(fig, use_container_width=True)

st.markdown("---")



# åˆ†æé€‰é¡¹å¡
tab1, tab3 = st.tabs([
    T('æ•…äº‹ç”Ÿæˆåˆ†æ', 'Story Generation Analysis'),
    T('å¯è¯»æ€§åˆ†æ', 'Readability Analysis')
])

with tab1:
    st.markdown(f"### {T('æ•…äº‹ç”Ÿæˆç»Ÿè®¡', 'Story Generation Statistics')}")
    
    if not stories_df.empty:
        col1, col2 = st.columns(2)
        
        with col1:
            # æŒ‰ä¸»é¢˜åˆ†å¸ƒ
            theme_counts = stories_df['theme'].value_counts()
            fig_theme = px.pie(
                values=theme_counts.values, 
                names=theme_counts.index,
                title=T('æ•…äº‹ä¸»é¢˜åˆ†å¸ƒ', 'Story Theme Distribution')
            )
            st.plotly_chart(fig_theme, use_container_width=True)
            
        with col2:
            # æŒ‰æç¤ºè¯é£æ ¼åˆ†å¸ƒ
            prompt_counts = stories_df['prompt_style'].value_counts()
            
            # æç¤ºè¯é£æ ¼ç¿»è¯‘æ˜ å°„
            prompt_style_translation = {
                'Structured': T('ç»“æ„åŒ–', 'Structured'),
                'Template': T('æ¨¡æ¿å¼', 'Template'), 
                'Question': T('é—®ç­”å¼', 'Question')
            }
            
            # åº”ç”¨ç¿»è¯‘åˆ°æ ‡ç­¾
            translated_names = [prompt_style_translation.get(name, name) for name in prompt_counts.index]
            
            fig_prompt = px.pie(
                values=prompt_counts.values,
                names=translated_names,
                title=T('æç¤ºè¯é£æ ¼åˆ†å¸ƒ', 'Prompt Style Distribution')
            )
            st.plotly_chart(fig_prompt, use_container_width=True)
        
        # æŒ‰å¹´é¾„æ®µåˆ†å¸ƒ
        age_counts = stories_df['age'].value_counts().sort_index()
        fig_age = px.bar(
            x=age_counts.index, 
            y=age_counts.values,
            title=T('ç›®æ ‡å¹´é¾„åˆ†å¸ƒ', 'Target Age Distribution'),
            labels={'x': T('å¹´é¾„', 'Age'), 'y': T('æ•…äº‹æ•°é‡', 'Story Count')}
        )
        st.plotly_chart(fig_age, use_container_width=True)
        

        
    else:
        st.info(T('æš‚æ— æ•…äº‹æ•°æ®å¯ä¾›åˆ†æ', 'No story data available for analysis'))




with tab3:
    st.markdown(f"### {T('å¯è¯»æ€§æŒ‡æ ‡åˆ†æ', 'Readability Metrics Analysis')}")
    
    if not stories_df.empty and 'flesch_score' in stories_df.columns:
        col1, col2 = st.columns(2)
        
        with col1:
            # Fleschåˆ†æ•°åˆ†å¸ƒ
            if 'flesch_score' in stories_df.columns and stories_df['flesch_score'].notna().sum() > 0:
                # è®¡ç®—åˆé€‚çš„binsæ•°é‡
                data_range = stories_df['flesch_score'].max() - stories_df['flesch_score'].min()
                bins = max(5, min(20, int(data_range / 5)))  # åŠ¨æ€è°ƒæ•´binsæ•°é‡
                
                fig_flesch = px.histogram(
                    stories_df, 
                    x='flesch_score',
                    nbins=bins,
                    title=T('Fleschå¯è¯»æ€§åˆ†æ•°åˆ†å¸ƒ', 'Flesch Readability Score Distribution'),
                    labels={'flesch_score': 'Flesch Score', 'count': T('æ•°é‡', 'Count')}
                )
                fig_flesch.update_layout(bargap=0.1)
                st.plotly_chart(fig_flesch, use_container_width=True)
            else:
                st.info(T('æš‚æ— Fleschåˆ†æ•°æ•°æ®', 'No Flesch score data available'))
            
        with col2:
            # è¯æ•°åˆ†å¸ƒ
            if 'word_count' in stories_df.columns and stories_df['word_count'].notna().sum() > 0:
                # è®¡ç®—åˆé€‚çš„binsæ•°é‡
                data_range = stories_df['word_count'].max() - stories_df['word_count'].min()
                bins = max(5, min(15, int(data_range / 20)))  # åŠ¨æ€è°ƒæ•´binsæ•°é‡
                
                fig_words = px.histogram(
                    stories_df, 
                    x='word_count',
                    nbins=bins,
                    title=T('æ•…äº‹è¯æ•°åˆ†å¸ƒ', 'Story Word Count Distribution'),
                    labels={'word_count': T('è¯æ•°', 'Word Count'), 'count': T('æ•°é‡', 'Count')}
                )
                fig_words.update_layout(bargap=0.1)
                st.plotly_chart(fig_words, use_container_width=True)
            else:
                st.info(T('æš‚æ— è¯æ•°æ•°æ®', 'No word count data available'))
        
        # å¯è¯»æ€§ä¸å¹´é¾„çš„å…³ç³»
        fig_age_readability = px.scatter(
            stories_df, 
            x='age', 
            y='flesch_score',
            color='model',
            title=T('å¹´é¾„ä¸å¯è¯»æ€§å…³ç³»', 'Age vs Readability Relationship'),
            labels={'age': T('ç›®æ ‡å¹´é¾„', 'Target Age'), 'flesch_score': 'Flesch Score'}
        )
        st.plotly_chart(fig_age_readability, use_container_width=True)
        
        # å„æç¤ºè¯é£æ ¼å¯è¯»æ€§å¯¹æ¯”
        if 'prompt_style' in stories_df.columns and len(stories_df['prompt_style'].unique()) > 1:
            prompt_readability = stories_df.groupby('prompt_style')['flesch_score'].agg(['mean', 'std', 'count']).reset_index()
            # å¤„ç†æ ‡å‡†å·®ä¸ºNaNçš„æƒ…å†µï¼ˆå•ä¸ªæ•°æ®ç‚¹ï¼‰
            prompt_readability['std'] = prompt_readability['std'].fillna(0)
            # åªæ˜¾ç¤ºæœ‰è¶³å¤Ÿæ•°æ®çš„ç»„
            prompt_readability = prompt_readability[prompt_readability['count'] >= 1]
            
            if not prompt_readability.empty:
                fig_prompt_readability = px.bar(
                    prompt_readability, 
                    x='prompt_style', 
                    y='mean',
                    error_y='std',
                    title=T('å„æç¤ºè¯é£æ ¼å¯è¯»æ€§å¯¹æ¯”', 'Readability Comparison by Prompt Style'),
                    labels={'prompt_style': T('æç¤ºè¯é£æ ¼', 'Prompt Style'), 'mean': T('å¹³å‡Fleschåˆ†æ•°', 'Average Flesch Score')}
                )
                st.plotly_chart(fig_prompt_readability, use_container_width=True)
            else:
                st.info(T('æç¤ºè¯é£æ ¼æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯¹æ¯”åˆ†æ', 'Insufficient prompt style data for comparison analysis'))
        else:
            st.info(T('éœ€è¦å¤šç§æç¤ºè¯é£æ ¼æ•°æ®æ‰èƒ½è¿›è¡Œå¯¹æ¯”åˆ†æ', 'Multiple prompt styles needed for comparison analysis'))
        
    else:
        st.info(T('æš‚æ— å¯è¯»æ€§æ•°æ®å¯ä¾›åˆ†æ', 'No readability data available for analysis'))





# st.markdown("---")

# # æ•°æ®å¯¼å‡º
# st.subheader(T("æ•°æ®å¯¼å‡º", "Data Export"))

# col1, col2 = st.columns(2)

# with col1:
#     if not stories_df.empty:
#         # åŸºç¡€CSVå¯¼å‡º
#         csv_stories = stories_df.to_csv(index=False)
#         st.download_button(
#             label=T("æ•…äº‹æ•°æ® (CSV)", "Stories Data (CSV)"),
#             data=csv_stories,
#             file_name=f"stories_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
#             mime="text/csv"
#         )
#         

#     else:
#         st.info(T("æš‚æ— æ•…äº‹æ•°æ®å¯å¯¼å‡º", "No story data available for export"))

# with col2:
#     st.info(T("åé¦ˆæ•°æ®å°†é€šè¿‡Google Formå•ç‹¬æ”¶é›†", "Feedback data will be collected separately via Google Form"))

# é¡µè„š
st.markdown("---")
st.markdown(
    f"<div style='text-align: center; color: #666; margin-top: 1rem;'>{T('æ•°æ®é©±åŠ¨çš„æ•…äº‹åˆ›ä½œä¼˜åŒ–', 'Data-Driven Story Creation Optimization')}</div>",
    unsafe_allow_html=True
)